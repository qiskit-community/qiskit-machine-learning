{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38df9aa0",
   "metadata": {},
   "source": [
    "# Efficient Parameter Optimisation for Quantum Kernel Alignment: A Sub-sampling Approach in Variational Training\n",
    "\n",
    "*The following tutorial has been built around the work by E. Sahin et al.* [[1]](https://arxiv.org/abs/2401.02879)\n",
    "\n",
    "The focus of this tutorial is on how to implement these methods with Qiskit and Qiskit machine learning, there are tasks throughout the notebook. The answers are hinted to but not there, any time you're stuck, first check the docs linked throughout, next ask one of the presenters for help and if neither of those suits you, just go to the completed version of this notebook with all these answers filled in. Keep in mind, if you do want to code along with us, you will need to fill out the answers whether you came to them yourselves or needed them revealed, naturally the notebook needs the lines filled in to run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8480a2-ab6a-4253-964f-d218a8d4fa30",
   "metadata": {},
   "source": [
    "![method scheme](img/schematic-view.jpg)\n",
    "\n",
    "**Figure 1 A)** Quantum kernel alignment uses n qubits to encode the dataset into a feature map, estimate the kernel,\n",
    "and optimize parameters θ until kernel separation improves. **B)** In the sub-sampling method, subsets with *k* elements of\n",
    "the dataset are similarly processed using *n* qubits. This cycle repeats with different subsets, optimizing θ until the entire\n",
    "dataset is sampled, yielding an optimized kernel. This will end with the estimation of the full dataset, optimised kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddc459-66de-49db-91aa-0eb87c173f18",
   "metadata": {},
   "source": [
    "### The notebook structure\n",
    "\n",
    "0. Import Local, External, and Qiskit Packages\n",
    "1. Prepare the dataset\n",
    "2. Quantum Fidelity Kernel classification\n",
    "3. Quantum Kernel Alignment\n",
    "4. The Sub-Kernel Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6aafa9",
   "metadata": {},
   "source": [
    "## 0. Import local, external, and Qiskit packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a646351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score\n",
    "from timeit import default_timer as timer\n",
    "from typing import Callable, Optional, Union, List, Dict, Any, Sequence\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "\n",
    "# Qiskit Machine Learning requirements\n",
    "from qiskit_machine_learning.kernels import (\n",
    "    FidelityStatevectorKernel,\n",
    "    TrainableKernel,\n",
    "    TrainableFidelityStatevectorKernel,\n",
    ")\n",
    "from qiskit_machine_learning.kernels.algorithms import QuantumKernelTrainer\n",
    "from qiskit_machine_learning.utils.loss_functions import SVCLoss\n",
    "from qiskit_machine_learning.algorithms.classifiers import QSVC\n",
    "\n",
    "# Qiskit algorithms requirements, soon to be a part of qiskit_machine_learning too\n",
    "from qiskit_algorithms.optimizers import SPSA, ADAM, NFT, GradientDescent, L_BFGS_B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39535c04",
   "metadata": {},
   "source": [
    "## 1. Prepare the Dataset\n",
    "\n",
    "We start by preparing the dataset. In this notebook, we use the the Labeling Cosets with Error (LCE) dataset [[2]](https://www.nature.com/articles/s41567-023-02340-9), this is group theory based synthetic dataset designed to evaluate quantum machine learning algorithm accuracies. We load the file `dataset_graph7.csv` from the Quantum Kernel Training (QKT) Toolkit [[3]](https://github.com/qiskit-community/prototype-quantum-kernel-training/tree/main/data). More information about the structure of this particular dataset is in the QKT Toolkit [[4]](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/background/qkernels_and_data_w_group_structure.ipynb). If we inspect the dataset, we see that each sample has 14 features and the labels are binary $\\pm1$.\n",
    "\n",
    "In the original paper [[1]](https://arxiv.org/abs/2401.02879), the method subkernel method was benchmarked on the Labeling Cosets with Error and Second order Pauli-Z evolution datasets and then finally on a real life breast cancer dataset. All of which benefitted from applying the methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e169c74-0a69-4dec-bf45-15a96f59b7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  (96, 14)\n",
      "training_labels:  (96,)\n",
      "test_data:  (32, 14)\n",
      "test_labels:  32\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(data_filepath, num_train=10, num_test=10, balanced=False):\n",
    "    \"\"\"\n",
    "    Quick function used to split data into test and training samples.\n",
    "\n",
    "    Args:\n",
    "        data_filepath (str): filepath to read the data from.\n",
    "        num_train (int): number of training samples.\n",
    "        num_test (int): number of test samples.\n",
    "        balanced (bool): whether there are an equal number of each class represented.\n",
    "\n",
    "    Returns:\n",
    "        X_train (np.ndarray): training sample features.\n",
    "        y_train (np.ndarray): testing sample features.\n",
    "        X_test (np.ndarray): training sample labels.\n",
    "        y_test (np.ndarray): testing sample labels.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(data_filepath, sep=\",\", header=None)\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    if balanced:\n",
    "        unique_labels = df.iloc[:, -1].unique()\n",
    "\n",
    "        for label in unique_labels:\n",
    "            label_indices = df[df.iloc[:, -1] == label].index.to_numpy()\n",
    "            np.random.shuffle(label_indices)\n",
    "\n",
    "            train_count = min(num_train, len(label_indices))\n",
    "            test_count = min(num_test, len(label_indices) - train_count)\n",
    "\n",
    "            train_indices.extend(label_indices[:train_count])\n",
    "            test_indices.extend(label_indices[train_count : train_count + test_count])\n",
    "\n",
    "    else:\n",
    "        indices = np.arange(len(df))\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_indices = indices[:num_train]\n",
    "        test_indices = indices[num_train : num_train + num_test]\n",
    "\n",
    "    X_train = df.iloc[train_indices, :-1].to_numpy()\n",
    "    y_train = df.iloc[train_indices, -1].to_numpy()\n",
    "\n",
    "    X_test = df.iloc[test_indices, :-1].to_numpy()\n",
    "    y_test = df.iloc[test_indices, -1].to_numpy()\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "# Load the dataset and split into train and test sets\n",
    "DATA_FILEPATH = \"data/dataset_graph7.csv\"\n",
    "training_data, training_labels, test_data, test_labels = train_test_split(\n",
    "    DATA_FILEPATH, num_train=96, num_test=32\n",
    ")\n",
    "# The proportion of train and test datapoints can be adjusted as needed.\n",
    "# Doing some quick checks.\n",
    "print(\"training_data: \", training_data.shape)\n",
    "print(\"training_labels: \", training_labels.shape)\n",
    "print(\"test_data: \", test_data.shape)\n",
    "print(\"test_labels: \", len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21274c57-72fd-4c6d-9d26-686e8b5b0842",
   "metadata": {},
   "source": [
    "## 2. Quantum Fidelity Kernel classification.\n",
    "Here we show how to create a Quantum Kernel (QK) in Qiskit and run a classification task using Qiskit machine learning's Quantum Support Vector Classifier (`QSVC`). Before we can create a QK, we must first design our feature map.\n",
    "\n",
    "\n",
    "### 2.a. The Covariant Feature Map\n",
    "\n",
    "We create the `CovariantFeatureMap` map taken from the QKT Toolkit. This is an extension of Qiskit's [`QuantumCircuit`](https://docs.quantum.ibm.com/api/qiskit/qiskit.circuit.QuantumCircuit). This feature map is tailored to the group structure of this particular dataset and is parameterised by a vector of parameters $\\mathbf{\\theta}$ of length equal to the number of qubits (feature dimension). For a deeper look into covariant quantum kernels for data with group structure consult the QKT Toolkit docs [[4]](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/background/qkernels_and_data_w_group_structure.ipynb). For more information on defining quantum feature maps in Qiskit, consult their guide on creating custom feature maps [[5]](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/how_tos/create_custom_quantum_feature_map.ipynb) and building trainable feature maps from existing circuits in Qiskit's circuit library [[6]](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/how_tos/train_kernels_using_qiskit_feature_maps.ipynb). \n",
    "\n",
    "To instantiate the `CovariantFeatureMap`, we specify:\n",
    "- Feature dimension\n",
    "- Entanglement structure of the quantum circuit\n",
    "- Type of parametrization for the trainable parameters\n",
    "\n",
    "In this example, we set the `feature_dimension` to that of the dataset. In addition, the `entanglement` parameter controls the structure of the fiducial state of the covariant quantum feature map [[4]](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/background/qkernels_and_data_w_group_structure.ipynb). We choose `entanglement` to match the graph used to generate the dataset (a subgraph of the heavy-hexagon lattice [[7]](https://www.ibm.com/blogs/research/2020/09/hardware-aware-quantum/)). With this selection, we expect our model to perform well on this dataset. However, other entanglement structures can be used if one does not have prior knowledge of what might constitute a good choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878aaa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovariantFeatureMap(QuantumCircuit):\n",
    "    \"\"\"The Covariant Feature Map circuit.\n",
    "\n",
    "    On 3 qubits and a linear entanglement, the circuit is represented by:\n",
    "\n",
    "    .. parsed-literal::A\n",
    "\n",
    "         ┌──────────────┐       ░ ┌─────────────────┐┌─────────────────┐\n",
    "    q_0: ┤ Ry(θ_par[0]) ├─■─────░─┤ Rz(-2*x_par[1]) ├┤ Rx(-2*x_par[0]) ├\n",
    "         ├──────────────┤ │     ░ ├─────────────────┤├─────────────────┤\n",
    "    q_1: ┤ Ry(θ_par[1]) ├─■──■──░─┤ Rz(-2*x_par[3]) ├┤ Rx(-2*x_par[2]) ├\n",
    "         ├──────────────┤    │  ░ ├─────────────────┤├─────────────────┤\n",
    "    q_2: ┤ Ry(θ_par[2]) ├────■──░─┤ Rz(-2*x_par[5]) ├┤ Rx(-2*x_par[4]) ├\n",
    "         └──────────────┘       ░ └─────────────────┘└─────────────────┘\n",
    "\n",
    "    where θ_par is a vector of trainable feature map parameters and x_par is a\n",
    "    vector of data-bound feature map parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dimension: int,\n",
    "        entanglement: Union[str, List[List[int]], Callable[[int], List[int]]] = None,\n",
    "        include_training_parameters: bool = True,\n",
    "        name: str = \"CovariantFeatureMap\",\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new Covariant Feature Map circuit.\n",
    "\n",
    "        Args:\n",
    "            feature_dimension (int): The number of features.\n",
    "            entanglement (str, List, Callable): Specifies the entanglement scheme.\n",
    "            include_training_parameters (bool): If True, includes trainable parameters, otherwise excludes them.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        if (feature_dimension % 2) != 0:\n",
    "            raise ValueError(\n",
    "                \"\"\"\n",
    "                Covariant feature map requires an even number of input features.\n",
    "                \"\"\"\n",
    "            )\n",
    "        self.feature_dimension = feature_dimension\n",
    "        self.entanglement = entanglement\n",
    "        self.include_training_parameters = include_training_parameters\n",
    "        self.training_parameters = None\n",
    "        self.input_parameters = None\n",
    "\n",
    "        num_qubits = feature_dimension // 2\n",
    "        super().__init__(num_qubits, name=name)\n",
    "\n",
    "        self._generate_feature_map()\n",
    "\n",
    "    @property\n",
    "    def settings(self) -> Dict[str, Any]:\n",
    "        training_parameters_list = [param for param in self.training_parameters]\n",
    "        input_parameters_list = [param for param in self.input_parameters]\n",
    "        return {\n",
    "            \"feature_dimension\": self.feature_dimension,\n",
    "            \"entanglement\": self.entanglement,\n",
    "            \"include_training_parameters\": self.include_training_parameters,\n",
    "            \"training_parameters\": training_parameters_list,\n",
    "            \"input_parameters\": input_parameters_list,\n",
    "        }\n",
    "\n",
    "    def _generate_feature_map(self):\n",
    "        # If no entanglement scheme specified, use linear entanglement\n",
    "        if self.entanglement is None:\n",
    "            self.entanglement = [[i, i + 1] for i in range(self.num_qubits - 1)]\n",
    "\n",
    "        # Vector of data parameters\n",
    "        input_params = ParameterVector(\"x_par\", self.feature_dimension)\n",
    "\n",
    "        if self.include_training_parameters:\n",
    "            training_params = ParameterVector(\"\\u03B8_par\", self.num_qubits)\n",
    "            # Create an initial rotation layer of trainable parameters\n",
    "            for i in range(self.num_qubits):\n",
    "                self.ry(training_params[i], self.qubits[i])\n",
    "            self.training_parameters = training_params\n",
    "        else:\n",
    "            self.training_parameters = []\n",
    "\n",
    "        self.input_parameters = input_params\n",
    "\n",
    "        # Create the entanglement layer\n",
    "        for source, target in self.entanglement:\n",
    "            self.cz(self.qubits[source], self.qubits[target])\n",
    "\n",
    "        self.barrier()\n",
    "\n",
    "        # Create a circuit representation of the data group\n",
    "        for i in range(self.num_qubits):\n",
    "            self.rz(-2 * input_params[2 * i + 1], self.qubits[i])\n",
    "            self.rx(-2 * input_params[2 * i], self.qubits[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929f0422",
   "metadata": {},
   "source": [
    "### Task 1.\n",
    "Have a crack at utilising the `CovariantFeatureMap` class. \n",
    "1. Create a version with trainable parameters and without trainable parameters.\n",
    "2. Try printing out the circuit using \\<circuit>.draw(\"mpl\"). \n",
    "3. Extension - try to print out the training parameters of the circuit with trainable parameters, this will be important later. Checkout the `QuantumCircuit` docs for help.\n",
    "\n",
    "We're starting small but don't be afraid to ask for assistance, we know some people here may be completely new to Qiskit even if they are adept in other QC compatible languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c8504d5-ab6a-4b98-8302-8621469a2ff0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_features = np.shape(training_data)[1]\n",
    "entangler_map = [[0, 2], [3, 4], [2, 5], [1, 4], [2, 3], [4, 6]]\n",
    "\n",
    "# Creating the feature map without trainable params\n",
    "# we will use this for an initial QSVC\n",
    "fm = \n",
    "\n",
    "# With trainable params\n",
    "fm_n =\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd94b3-fc38-490a-9e19-bee24efe7df1",
   "metadata": {},
   "source": [
    "### 2.b. Performing a classiciation task using a QSVC.\n",
    "\n",
    "As mentioned previously, to utilise a kernel, classical or quantum we need to perform a classication algorithm on it. Here, we get our first taste of what the Qiskit machine learning package can do - we can perform a `QSVC` using a built in [`QSVC`](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.algorithms.QSVC.html) feature, which is an extension of sklearns SVC. The `QSVC` is initialised with a QK and when the fit method is called, it calls the QK to evaluate itself and then passes the resultant kernel to the SVC fit method.\n",
    "\n",
    "We create a `run_QSVC` function to create and fit our `QSVC` as well as calculate some popular machine learning metrics (ROC AUC, F1, Test accuracy) to evaluate training success.\n",
    "\n",
    "### Task 2.\n",
    "1. Create a `QSVC` object (make sure to add the `probability = True` attribute so we can calculate probability outputs)\n",
    "2. Add a line that fits the `QSVC`.\n",
    "\n",
    "Consult the docs for `QSVC` for help on creating these lines or simply ask one of us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1992310-1672-4664-ab72-8a6f15fd8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_QSVC(training_input, train_labels, test_input, test_labels, quantum_kernel):\n",
    "    \"\"\"A function to fit and score a quantum support vector classifier.\n",
    "\n",
    "    Args:\n",
    "        training_input (np.ndarray): training sample features.\n",
    "        train_labels (np.ndarray): training sample labels.\n",
    "        test_input (np.ndarray): testing sample features.\n",
    "        test_labels (np.ndarray): testing sample labels.\n",
    "\n",
    "    Returns:\n",
    "        qsvc (QSVC): the fitted qsvc object.\n",
    "        auc (float): the ROC AUC score.\n",
    "        f1 (float): the f1 score.\n",
    "    \"\"\"\n",
    "    # Create the QSVC\n",
    "    qsvc = \n",
    "\n",
    "    # Fit the QSVC\n",
    "    \n",
    "\n",
    "    # predict probabilities\n",
    "    predicted = qsvc.predict(test_input)\n",
    "    predicted_proba = qsvc.predict_proba(test_input)\n",
    "    # Calculate F1-Score and write\n",
    "    f1 = f1_score(test_labels, predicted, average=\"weighted\")\n",
    "    # calculate scores\n",
    "    try:\n",
    "        auc = roc_auc_score(\n",
    "            test_labels, predicted_proba, multi_class=\"ovo\", average=\"weighted\"\n",
    "        )\n",
    "    except:\n",
    "        auc = roc_auc_score(test_labels, predicted_proba[:, 1])\n",
    "    \n",
    "    acc_score = accuracy_score(test_labels, predicted)\n",
    "    # summarize scores\n",
    "    return qsvc, auc, f1, acc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001ebbd2-1861-451f-a9b3-62bc9b509917",
   "metadata": {},
   "source": [
    "Now to run our first experiment. Before running the `QSVC` we must create our QK, we will utilise Qiskit machine learning's [`FidelityStatevectorKernel`](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.kernels.FidelityStatevectorKernel.html#fidelitystatevectorkernel), this is an implementation of the `BaseKernel` which is optimised for simulated statevectors. `FidelityStatevectorKernel` is initialised with a feature map and implements a quantum kernel based on the state fidelity. As a reminder, the kernel between two classical inputs $x_i$ and $x_j$ is given by:\n",
    "$$ K(x_i, x_j) = | \\langle \\phi(x_i) | \\phi(x_j) \\rangle |^2 $$\n",
    "where $|\\phi (x_i)\\rangle = U_{\\phi}(x_i) |0\\rangle^{\\otimes n}$ and $U_{\\phi}(x_i)$ is the unitary that implements the quantum feature map.\n",
    "\n",
    "\n",
    "### Task 3\n",
    "1. Create a variable `fidelity_kernel` and initialise it as a `FidelityStatevectorKernel` object with the covariant feature map with __no trainable parameters__ (fm_n).\n",
    "2. Run the `run_QSVC` function to show a `QSVC` in Qiskit machine learning.\n",
    "3. Extension - evaluate the kernel on the `training_data` and uncomment the plotting lines to visualise its output.\n",
    "\n",
    "The results of the `QSVC` will indicate a poorly trained model, with all metrics showing weak results. Additionally the kernel sees little difference between fidelity for a majority of the dataset, this in part leads to the poor performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3dabf653-7b34-4d9f-b58c-1a162589c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Quantum Fidelity kernel...\n",
      "QSVC Training Runtime: 0.19731033299467526 secs\n",
      "Quantum Fidelity Kernel F1 Score = 0.239\n",
      "Quantum Fidelity Kernel ROC AUC = 0.591\n",
      "Quantum Fidelity Kernel Accuracy Score = 0.375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fidelity_kernel = \n",
    "print(\"Evaluating Quantum Fidelity kernel...\")\n",
    "# Evaluate the quantum fidelity kernel\n",
    "start = timer()\n",
    "qsvc, auc, f1, accuracy = run_QSVC(\n",
    "    training_data, training_labels, test_data, test_labels, fidelity_kernel\n",
    ")\n",
    "end = timer()\n",
    "qsvc_runtime = end - start\n",
    "print(f\"QSVC Training Runtime: {qsvc_runtime} secs\")\n",
    "# Print results\n",
    "print(\"Quantum Fidelity Kernel F1 Score = %.3f\" % (f1))\n",
    "print(\"Quantum Fidelity Kernel ROC AUC = %.3f\" % (auc))\n",
    "print(\"Quantum Fidelity Kernel Accuracy Score = %.3f\" % (accuracy))\n",
    "print()\n",
    "\n",
    "# Visualising the untrained kernel\n",
    "# untrained_kernel_eval = \n",
    "# plt.imshow(untrained_kernel_eval, cmap=matplotlib.colormaps[\"bwr\"])\n",
    "# plt.colorbar(label=\"fidelity\")\n",
    "# plt.title(\"Untrained kernel\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0811b4e",
   "metadata": {},
   "source": [
    "## 3. Quantum Kernel Alignment\n",
    "\n",
    "Considering the poor performance of the `QSVC`, we now look at find a way of improving the kernel, this is where Quantum Kernel Alignment (QKA) comes in. QKA adds a variational layer $U(\\boldsymbol{\\theta})$ to the kernel, where $\\boldsymbol{\\theta}$ are trainable parameters. The elements of the fidelity kernel matrix are computed in the same way, but now depend on $\\boldsymbol{\\theta}$ as shown below:\n",
    "\n",
    "$$ K(x_i, x_j, \\boldsymbol{\\theta}) =  |\\langle\\psi (\\boldsymbol{\\theta}, x_i)|\\psi (\\boldsymbol{\\theta}, x_j)\\rangle |^2 $$\n",
    "where $|\\psi (\\boldsymbol{\\theta}, x_i)\\rangle = U_{\\phi}(x_i) U(\\boldsymbol{\\theta}) |0\\rangle^{\\otimes n} $.\n",
    "\n",
    "By adding training parameters to the feature map, the kernel can be optimised for the dataset. QKA aims to maximize the intra-class overlap and minimize the inter-class overlap [[2]](https://arxiv.org/pdf/2105.03406) whilst giving an upper bound on the generalisation error, allowing the SVC to generalise well and not overfit [[8]](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/background/svm_weighted_kernel_alignment.ipynb).\n",
    "\n",
    "### 3.a. Loss and Callback\n",
    "\n",
    "To fit the kernel we need a metric for knowing how well the current parameters $\\boldsymbol\\theta$ are performing. Here we use Qiskit machine learning's [`SVCLoss`](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.utils.loss_functions.SVCLoss.html). Given training samples, $x_{i}$, with binary labels, $y_{i}$ and a parameterised kernel, $K(\\boldsymbol\\theta,x_i, x_j)$ parameterized by values, $\\boldsymbol\\theta$, the loss is defined as:\n",
    "    \n",
    "$$ \\text{SVCLoss} = \\sum_{i} a_i - 0.5 \\sum_{i,j} a_i a_j y_{i} y_{j} K(\\boldsymbol\\theta,x_i, x_j) $$\n",
    "\n",
    "where $a_i$ are the optimal Lagrange multipliers found by solving the standard SVM quadratic program. Note that the hyper-parameter `C` for the soft-margin penalty can be specified through the keyword args.\n",
    "\n",
    "Minimizing this loss over the parameters, $\\boldsymbol\\theta$, of the kernel is equivalent to maximizing a weighted kernel alignment, which in turn yields the smallest upper bound to the SVM generalization error for a given parameterization. See [[2]](https://arxiv.org/abs/2105.03406) for further details.\n",
    "\n",
    "Before performing the QKA, we need to define our optimisers and callback function. Callback functions are a feature of Qiskit machine learning and are used to provide feedback during the optimisation process. The `QKTCallback` class below is an implementation of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8cc5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKTCallback:\n",
    "    \"\"\"Callback wrapper class.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"Initialise the class with a data array.\"\"\"\n",
    "        self._data = [[] for i in range(5)]\n",
    "\n",
    "    def callback(\n",
    "        self,\n",
    "        n_func_iter,\n",
    "        weights=None,\n",
    "        func_value=None,\n",
    "        step_size=None,\n",
    "        step_accepted=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args\n",
    "            n_func_iter (int): number of function evaluations.\n",
    "            weights (np.ndarray): the current weights.\n",
    "            func_value (float): the function value.\n",
    "            step_size (float): the step size.\n",
    "            step_accepted (bool): whether the step was accepted.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self._data[0].append(n_func_iter)\n",
    "        self._data[1].append(weights)\n",
    "        self._data[2].append(func_value)\n",
    "        self._data[3].append(step_size)\n",
    "        self._data[4].append(step_accepted)\n",
    "\n",
    "    def get_callback_data(self):\n",
    "        return self._data\n",
    "\n",
    "    def clear_callback_data(self):\n",
    "        self._data = [[] for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133c712",
   "metadata": {},
   "source": [
    "### 3.b. Optimizer Helper Function\n",
    "Now we need something to do the hard work: an opimiser. These optimisers originate from qiskit_algorithms but will soon join the Qiskit machine learning library. To optimize the parameters of our quantum circuit, we mostly use gradient-based optimization methods. There are several optimization methods to choose from, and the choice of the optimizer can significantly influence the speed of convergence and the quality of the final solution. For this notebook we will focus on the `SPSA` optimiser, though feel free to try some others later on.\n",
    "\n",
    "The get_optimizer_options function presented below allows us to flexibly choose from various optimization methods available in the Qiskit SDK [[9]](https://qiskit.org/documentation/stubs/qiskit.algorithms.optimizers.html) and returns the chosen optimizer function using partial to pass some optimised parameters found. We provide support for the following optimizers:\n",
    "\n",
    "* [Simultaneous Perturbation Stochastic Approximation](https://qiskit.org/documentation/stubs/qiskit.algorithms.optimizers.SPSA.html#qiskit.algorithms.optimizers.SPSA) (SPSA): SPSA is an gradient descent method for optimizing systems with multiple unknown parameters. As an optimization method, it is appropriately suited to large-scale population models, adaptive modeling, and simulation optimization.\n",
    "\n",
    "* [Adaptive Moment Estimation](https://qiskit.org/documentation/stubs/qiskit.algorithms.optimizers.ADAM.html#qiskit.algorithms.optimizers.ADAM) (ADAM): ADAM is an extension of stochastic gradient descent and is one of the default optimizers in many deep learning libraries. It is a gradient-based optimization algorithm that is relies on adaptive estimates of lower-order moments. The algorithm requires little memory and is invariant to diagonal rescaling of the gradients. Furthermore, it is able to cope with non-stationary objective functions and noisy and/or sparse gradients.\n",
    "\n",
    "* [Gradient Descent](https://qiskit.org/documentation/stubs/qiskit.algorithms.optimizers.GradientDescent.html#qiskit.algorithms.optimizers.GradientDescent) (GD): GD is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. It works by taking steps proportional to the negative of the gradient at the current point. \n",
    "\n",
    "* [Nakanishi-Fujii-Todo](https://qiskit.org/documentation/stubs/qiskit.algorithms.optimizers.NFT.html#qiskit.algorithms.optimizers.NFT) algorithm (NFT): The NFT is a sequential minimal optimization method for quantum-classical hybrid algorithms, which converges faster, is robust against statistical error, and is hyperparameter-free [[10]](https://arxiv.org/abs/1903.12166).\n",
    "\n",
    "* [Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS-B)](https://qiskit.org/documentation/stubs/qiskit.algorithms.optimizers.L_BFGS_B.html#qiskit.algorithms.optimizers.L_BFGS_B): The L-BFGS-B is an algorithm for solving nonlinear optimization problems. It is a member of the broad class of quasi-Newton optimization methods. L-BFGS-B uses a limited amount of computer memory to approximate the inverse Hessian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47c27cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_options(optimizer_type):\n",
    "    if optimizer_type == \"SPSA\":\n",
    "        optimizer = partial(SPSA, maxiter=300, learning_rate=0.002, perturbation=0.05)\n",
    "        callback = \"SPSACallback\"\n",
    "    elif optimizer_type == \"ADAM\":\n",
    "        optimizer = partial(ADAM, maxiter=600, tol=1e-08, lr=0.002)\n",
    "        callback = None\n",
    "    elif optimizer_type == \"GradientDescent\":\n",
    "        optimizer = partial(\n",
    "            GradientDescent,\n",
    "            maxiter=400,\n",
    "            learning_rate=0.002,\n",
    "            tol=1e-08,\n",
    "            perturbation=None,\n",
    "        )\n",
    "        callback = \"SPSACallback\"\n",
    "    elif optimizer_type == \"NFT\":\n",
    "        optimizer = partial(NFT, disp=False)\n",
    "        callback = None\n",
    "    elif optimizer_type == \"L_BFGS_B\":\n",
    "        optimizer = partial(\n",
    "            L_BFGS_B,\n",
    "            maxfun=15000,\n",
    "            maxiter=15000,\n",
    "            ftol=2.220446049250313e-15,\n",
    "            iprint=-1,\n",
    "            eps=1e-08,\n",
    "        )\n",
    "        callback = None\n",
    "    return callback, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d4d5a",
   "metadata": {},
   "source": [
    "### 3.c. Experiment and Visualisation\n",
    "\n",
    "To train the quantum kernel on the dataset (samples and labels), we use the Qiskit machine learning [`QuantumKernelTrainer`](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.kernels.algorithms.QuantumKernelTrainer.html#qiskit_machine_learning.kernels.algorithms.QuantumKernelTrainer) (QKT). This will take in a `TrainableKernel` object which is contains trainable kernel parameters, we use [`TrainableFidelityStatevectorKernel`](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.kernels.TrainableFidelityStatevectorKernel.html#qiskit_machine_learning.kernels.TrainableFidelityStatevectorKernel), a trainable version of `FidelityStatevectorKernel`. The QKT also needs a loss function, we pass \"svc_loss\" to use the `SVCLoss` previously described. The QKA is performed by optimising the variational parameters using the `.fit` method.\n",
    "\n",
    "The output of `QuantumKernelTrainer.fit` is a [`QuantumKernelTrainerResult`](https://qiskit-community.github.io/qiskit-machine-learning/stubs/qiskit_machine_learning.kernels.algorithms.QuantumKernelTrainerResult.html#qiskit_machine_learning.kernels.algorithms.QuantumKernelTrainerResult) object. The results object contains the following class fields:\n",
    " - `optimal_parameters`: A dictionary containing {parameter: optimal value} pairs\n",
    " - `optimal_point`: The optimal parameter value found in training\n",
    " - `optimal_value`: The value of the loss function at the optimal point\n",
    " - `optimizer_evals`: The number of evaluations performed by the optimizer\n",
    " - `optimizer_time`: The amount of time taken to perform optimization\n",
    " - `quantum_kernel`: A `TrainableKernel` object with optimal values bound to the feature map\n",
    "\n",
    " ### Task 4.\n",
    " 1. Initialise a `TrainableFidelityStatevectorKernel` quantum kernel. This time you will need to use the feature map that includes the trainable parameters and you will need to specify those parameters.\n",
    " 2. Initilaise a `QuantumKernelTrainer` object using your quantum kernel. Here you'll need to specify the loss, optimizer and initial point for reproducable results.\n",
    " 3. Fit the `QuantumKernelTrainer` using the `.fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc193c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate quantum kernel\n",
    "quant_kernel = \n",
    "\n",
    "# Set up the optimizer\n",
    "optimizer = \"SPSA\"\n",
    "cb, optimizer = get_optimizer_options(optimizer)\n",
    "if cb == \"SPSACallback\":\n",
    "    callb = QKTCallback()\n",
    "    optimizer = optimizer(callback=callb.callback)\n",
    "else:\n",
    "    optimizer = optimizer()\n",
    "\n",
    "init_p = 0.25 * np.random.uniform(\n",
    "    -np.pi, np.pi, len(fm.training_parameters)\n",
    ")  \n",
    "# Instantiate a quantum kernel trainer.\n",
    "qkt = \n",
    "# Train the kernel\n",
    "start = timer()\n",
    "# Run the fit method\n",
    "qka_results = \n",
    "end = timer()\n",
    "train_time = end - start\n",
    "print(f\" Training Runtime: {train_time} secs. Results: \")\n",
    "print('-'*80)\n",
    "print()\n",
    "# print(qka_results)\n",
    "\n",
    "# Evaulating the optimised kernel on the training data\n",
    "trained_kernel_eval = qka_results.quantum_kernel.evaluate(training_data) \n",
    "\n",
    "# Graphing\n",
    "if callb is not None:\n",
    "    plot_data = callb.get_callback_data()  # callback data\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    ax[0].plot([i + 1 for i in range(len(plot_data[0]))], np.array(plot_data[2]), c=\"k\", marker=\"\")\n",
    "    ax[0].set_xlabel(\"Iterations\")\n",
    "    ax[0].set_ylabel(\"Loss\")\n",
    "    ax[0].set_title(\"Loss during training\")\n",
    "    im = ax[1].imshow(trained_kernel_eval, cmap=matplotlib.colormaps[\"bwr\"])\n",
    "    ax[1].set_title(\"Trained Kernel\")\n",
    "    fig.colorbar(im, ax=ax, label=\"fidelity\")\n",
    "    plt.show()\n",
    "else:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    ax[0].imshow(trained_kernel_eval, cmap=matplotlib.colormaps[\"bwr\"])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3038fe3",
   "metadata": {},
   "source": [
    "We can see the above loss graph shows us the loss has converged and the outputted trained kernel has much more contrast than our untrained kernel. This is the action of increasing intra-class fidelities and decreasing inter-class fidelities. We can next run a `QSVC` to see the increase in performance when using the Aligned QK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c0808f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the quantum fidelity kernel\n",
    "start = timer()\n",
    "qsvc, auc, f1, accuracy = run_QSVC(\n",
    "    training_data, training_labels, test_data, test_labels, qka_results.quantum_kernel\n",
    ")\n",
    "end = timer()\n",
    "qsvc_runtime = end - start\n",
    "print(f\"QSVC Training Runtime: {qsvc_runtime} secs\")\n",
    "# Print results\n",
    "print(\"Quantum Fidelity Kernel F1 Score = %.3f\" % (f1))\n",
    "print(\"Quantum Fidelity Kernel ROC AUC = %.3f\" % (auc))\n",
    "print(\"Quantum Fidelity Kernel Accuracy Score = %.3f\" % (accuracy))\n",
    "print(\n",
    "    \"Increase in overall time by aligning kernel = %.3f\"\n",
    "    % ((train_time + qsvc_runtime) * 100 / qsvc_runtime),\n",
    "    \"%\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1fd591",
   "metadata": {},
   "source": [
    "The `QSVC` metrics are stellar across the board and the aligned QK is much successful at the classification problem. However, aligning the kernel increases runtime by a signficant margin, this increase is exactly correlated to the number of optimiser iterations. This is a steep cost for the increase in performance as the kernel calculation is already _quadratic_ with number of features and this must be done at each iteration step. Additionally, there are are other considerations with QKs like shot noise, higher error rates and the need for error mitigation or correction. This neatly brings us to the sub-kernel method, a way of reducing computational burden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92cce71-b961-4ba9-9a0c-6318b10dde57",
   "metadata": {},
   "source": [
    "## 4. The Sub-Kernel Method\n",
    "### 4.a. How to Implement\n",
    "\n",
    "The sub-kernel methodology is an adaptation of the QFK method which uses sub-portions of the dataset to calculate a sub-kernel. The core idea behind the sub-kernel methodology is to randomly select a subset of the data points, referred to as a \"sub-kernel\", on each step of the training process until optimal parameters are found. \n",
    "\n",
    "In mathematical terms, , we calculate the kernel function only for the points in $\\mathcal{D'}$ in each iteration.\n",
    "\n",
    "Formally, if $\\mathcal{D}$ is our dataset and $\\mathcal{D'}$ is a randomly sampled subset of $\\mathcal{D}$ for each iteration $i$, we sample a subset $\\mathcal{D}_i' \\subset \\mathcal{D}$ of size $k \\ll m$, where $m$ is the size of the full dataset. Then we compute the fidelity kernel $\\mathbf{K}_{\\mathcal{D}_i'}$ on the subset $\\mathcal{D}_i'$ and update the parameters of our variational parameteres accordingly. This process continues iteratively until given conditions met. \n",
    "\n",
    "This method aims at reducing the computational cost of kernel construction. Despite this reduction, it preserves the essential features of the data, and thus, maintains good classification performance.\n",
    "\n",
    "To realise the sub-kernel method in Qiskit, we must create two additional classes. The first is the `DataBatcher` class, used to generate a list of balanced batches, where each batch contains the same number of samples from each label or a list of imbalanced batches, where each batch may contain a different number of samples from each label with given options. This function creates set of $\\mathcal{D'}$ from $\\mathcal{D}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1db3eb26-17bf-4531-aa34-a09ec2eb39ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DataBatcher:\n",
    "    \"\"\"\n",
    "    A class used to batch dataset and labels.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, labels):\n",
    "        \"\"\"\n",
    "        Initialize a DataBatches object with the input dataset and corresponding labels.\n",
    "\n",
    "        Args:\n",
    "            dataset (numpy array): A numpy array of shape (num_samples, num_features) containing the input dataset.\n",
    "            labels (numpy array): A numpy array of shape (num_samples,) containing the corresponding labels for the dataset.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.labels = labels\n",
    "        self.num_samples = len(dataset)\n",
    "        self.unique_labels, self.label_counts = np.unique(labels, return_counts=True)\n",
    "\n",
    "    def balanced_batches(self, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Generate a list of balanced batches, where each batch contains the same number of samples from each label.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The desired size of each batch.\n",
    "            shuffle (bool): if True, shuffle batches.\n",
    "\n",
    "        Returns:\n",
    "            batches (List): a list of batches where each batch is a tuple containing the batch data and corresponding labels.\n",
    "        \"\"\"\n",
    "        if batch_size > self.num_samples:\n",
    "            raise ValueError(\n",
    "                f\"Batch size {batch_size} is larger than the dataset size {self.num_samples}\"\n",
    "            )\n",
    "        if batch_size > 2 * np.min(self.label_counts):\n",
    "            raise ValueError(\n",
    "                f\"Batch size {batch_size} is 2x larger than the smallest label size {np.min(self.label_counts)}\"\n",
    "            )\n",
    "        samples_per_label = batch_size // len(self.unique_labels)\n",
    "        batches = []\n",
    "        for _ in range(self.num_samples // batch_size):\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "            for l in self.unique_labels:\n",
    "                label_indices = np.where(self.labels == l)[0]\n",
    "                if shuffle:\n",
    "                    np.random.shuffle(label_indices)\n",
    "                if samples_per_label > len(label_indices):\n",
    "                    batch_indices = label_indices\n",
    "                else:\n",
    "                    batch_indices = label_indices[:samples_per_label]\n",
    "                batch_data.append(self.dataset[batch_indices])\n",
    "                batch_labels.append(self.labels[batch_indices])\n",
    "            batch_data = np.concatenate(batch_data, axis=0)\n",
    "            batch_labels = np.concatenate(batch_labels, axis=0)\n",
    "            batches.append((batch_data, batch_labels))\n",
    "        return batches\n",
    "\n",
    "    def imbalanced_batches(self, batch_size, keep_ratio=False, shuffle=False):\n",
    "        \"\"\"\n",
    "        Generate a list of imbalanced batches, where each batch may contain a different number of samples from each label.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The desired size of each batch.\n",
    "            keep_ratio (bool): If True, maintain the same relative frequency of each label as in the original dataset.\n",
    "                            If False, use the absolute frequency of each label to determine the number of samples per label.\n",
    "            shuffle (bool): If True, shuffle batches.\n",
    "\n",
    "        Returns:\n",
    "            batches (list): a list of batches where each batch is a tuple containing the batch data and corresponding labels.\n",
    "        \"\"\"\n",
    "        if batch_size > self.num_samples:\n",
    "            raise ValueError(\n",
    "                f\"Batch size {batch_size} is larger than the dataset size {self.num_samples}\"\n",
    "            )\n",
    "        if keep_ratio:\n",
    "            # calculate the number of samples per label based on the relative label frequencies\n",
    "            label_freqs = self.label_counts / np.sum(self.label_counts)\n",
    "            samples_per_label = np.round(batch_size * label_freqs).astype(int)\n",
    "        else:\n",
    "            # calculate the number of samples per label based on the absolute label frequencies\n",
    "            samples_per_label = np.round(\n",
    "                batch_size * self.label_counts / np.sum(self.label_counts)\n",
    "            ).astype(int)\n",
    "        batches = []\n",
    "        for _ in range(self.num_samples // batch_size):\n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "            for l, num_samples in zip(self.unique_labels, samples_per_label):\n",
    "                label_indices = np.where(self.labels == l)[0]\n",
    "                if shuffle:\n",
    "                    np.random.shuffle(label_indices)\n",
    "                batch_indices = label_indices[:num_samples]\n",
    "                batch_data.append(self.dataset[batch_indices])\n",
    "                batch_labels.append(self.labels[batch_indices])\n",
    "            batch_data = np.concatenate(batch_data, axis=0)\n",
    "            batch_labels = np.concatenate(batch_labels, axis=0)\n",
    "            batches.append((batch_data, batch_labels))\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234f5934",
   "metadata": {},
   "source": [
    "The second is the `BatchedSVCLoss` class. This is an extension of Qiskit machine learning `SVCLoss` and provides a kernel loss function for multiple batches of the sub-kernels. It calculates and updates the loss based of each of these classification tasks by fitting an SVC model from scikit-learn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "387aca47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchedSVCLoss(SVCLoss):\n",
    "    r\"\"\"\n",
    "    This class provides a kernel loss function for classification tasks by fitting an ``SVC`` model\n",
    "    from scikit-learn, extended for use with batches. Given training samples, :math:`x_{i}`, with binary labels, :math:`y_{i}`,\n",
    "    and a kernel, :math:`K_{θ}`, parameterized by values, :math:`θ`, the loss is defined as:\n",
    "\n",
    "    .. math::\n",
    "\n",
    "        SVCLoss = \\sum_{i} a_i - 0.5 \\sum_{i,j} a_i a_j y_{i} y_{j} K_θ(x_i, x_j)\n",
    "\n",
    "    where :math:`a_i` are the optimal Lagrange multipliers found by solving the standard SVM\n",
    "    quadratic program. Note that the hyper-parameter ``C`` for the soft-margin penalty can be\n",
    "    specified through the keyword args.\n",
    "\n",
    "    Minimizing this loss over the parameters, :math:`θ`, of the kernel is equivalent to maximizing a\n",
    "    weighted kernel alignment, which in turn yields the smallest upper bound to the SVM\n",
    "    generalization error for a given parameterization.\n",
    "\n",
    "    See https://arxiv.org/abs/2105.03406 for further details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "        sub_kernel_size: Optional[int] = None,\n",
    "        minibatch_size: Optional[int] = 1,\n",
    "        shuffle: bool = False,\n",
    "        balanced_batch: bool = False,\n",
    "        keep_ratio: bool = True,\n",
    "        encoder=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (np.ndarray): The data to evaluate the loss on.\n",
    "            labels (np.ndarray): The corresponding labels for the data.\n",
    "            sub_kernel_size (int, optional): The size of the sub-kernel batches to split the data into. If not provided,\n",
    "                the entire data set is used in a single batch.\n",
    "            shuffle (bool, optional): Whether to shuffle the data before splitting into batches. Default is False.\n",
    "            balanced_batch (bool, optional): Whether to use balanced or imbalanced batching. Default is False.\n",
    "            encoder (torch.nn): An instance to optionally reduce dimension before calculating loss\n",
    "            **kwargs: Arbitrary keyword arguments to pass to SVC constructor within\n",
    "                      SVCLoss evaluation.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        # Split data into batches\n",
    "        self.sub_kernel_size = sub_kernel_size\n",
    "        bg = DataBatcher(data, labels)\n",
    "        self.minibatch_size = minibatch_size\n",
    "        if self.sub_kernel_size == None:\n",
    "            self.batches = [data, labels]\n",
    "        elif balanced_batch:\n",
    "            self.batches = bg.balanced_batches(sub_kernel_size, shuffle=shuffle)\n",
    "        else:\n",
    "            self.batches = bg.imbalanced_batches(\n",
    "                sub_kernel_size, keep_ratio=keep_ratio, shuffle=shuffle\n",
    "            )\n",
    "\n",
    "        self.idx = 0\n",
    "        self.epoch = 0\n",
    "        self.encoder = encoder\n",
    "        self.loss_arr = []\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        parameters: Sequence[float],\n",
    "        quantum_kernel: TrainableKernel,\n",
    "        data: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Wrapper function for loss evaluation with batches of data. If sub_kernel_size is None, it will execute SVCLoss() on full dataset.\n",
    "\n",
    "        Args:\n",
    "            parameter_values (Sequence[float]): The parameter values to evaluate the loss with.\n",
    "            quantum_kernel (TrainableKernel): The quantum kernel to use for evaluation.\n",
    "        Returns:\n",
    "            loss (float): the loss value for the given parameters and quantum kernel.\n",
    "        \"\"\"\n",
    "        if self.sub_kernel_size == None:\n",
    "            if type(self.encoder) != type(None):\n",
    "                weights = parameters[: self.encoder.num_weights]\n",
    "                variational_params = parameters[self.encoder.num_weights :]\n",
    "                self.encoder.set_weights(weights)\n",
    "                encoded_data = self.encoder.encode(data)\n",
    "                return super().evaluate(variational_params, quantum_kernel, encoded_data, labels)\n",
    "            else:\n",
    "                loss = super().evaluate(parameters, quantum_kernel, data, labels)\n",
    "                self.loss_arr.append(loss)\n",
    "                return loss\n",
    "\n",
    "        if self.idx + self.minibatch_size > len(self.batches):\n",
    "            self.idx = 0\n",
    "            self.epoch += 1\n",
    "\n",
    "        mini_batch = self.batches[self.idx : self.idx + self.minibatch_size]\n",
    "        # Evaluate the loss for each batch and accumulate the total loss\n",
    "        total_loss = 0\n",
    "        i = self.idx\n",
    "        for batch_data, batch_labels in mini_batch:\n",
    "            if type(self.encoder) != type(None):\n",
    "                weights = parameters[: self.encoder.num_weights]\n",
    "                variational_params = parameters[self.encoder.num_weights :]\n",
    "                self.encoder.set_weights(weights)\n",
    "                batch_data = self.encoder.encode(batch_data)\n",
    "            else:\n",
    "                variational_params = parameters\n",
    "            loss = super().evaluate(variational_params, quantum_kernel, batch_data, batch_labels)\n",
    "            total_loss += loss\n",
    "            i += 1\n",
    "        self.idx += self.minibatch_size\n",
    "        self.loss_arr.append(total_loss / self.minibatch_size)\n",
    "        param_loss = total_loss / self.minibatch_size\n",
    "        return param_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ca8e23-9b1d-4ab1-bb78-3647b32a5723",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 4.b. Sub-Kernel Experiments\n",
    "\n",
    "This where we pull all the previous pieces together to align QFK using full and different-sized Sub-Kernels with different optimizers. The core of the algorithm is as follows.\n",
    "\n",
    "- *Variational parameter initialisation:* the variational parameters $\\boldsymbol{\\theta}$ of the quantum circuit are initialised as $\\boldsymbol{\\theta}_{init}$, giving an initial state $|\\phi(\\boldsymbol{\\theta}_{init})\\rangle$.\n",
    "- *Variational parameters and loss function optimisation loop:* execute the optimisation loop and for each iteration, execute the following sub-steps:\n",
    "    - *Dataset subsampling:* initiate by randomly selecting a distinct subset of $k$ data points from the full dataset and employ the kernel equation to construct a sub kernel, $K_{\\text{sub}}$. Note that each time a new subset is selected, it is composed of previously unselected points until such a time that the entire dataset is exhausted (depending on the sub kernel and batch size), at which point previously selected points may be used again.\n",
    "    - *Loss function estimation:* compute the loss function $L$ using $K_{\\text{sub}}$.\n",
    "    - *Loss function optimisation:* perform an optimisation step on $L$ to update the parameters $\\boldsymbol{\\theta}$.\n",
    "- *Full kernel estimation:* utilise the optimised parameters $\\boldsymbol{\\theta}_{opt}$ to compute the full quantum kernel $K_f(\\boldsymbol{\\theta}_{opt})$.\n",
    "- *Training of classifier:* implement the full kernel in an SVM model for binary classification.\n",
    "\n",
    "This methodology can potentially decrease the computational burden associated with training on the full kernel, whilst retaining the classification accuracy as a minimum performance.  The exact computational savings will depend on the specific distribution of data and the convergence behaviour of the training process. As such, while sub-kernel training can potentially reduce the computational burden, careful consideration must be given to the selection of sub-kernel size and the resulting impact on convergence rate.\n",
    "\n",
    "Below is the plotting function we use to display the QKA loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ea589b40-dc2b-45c3-b182-fac47072f8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_average_loss_with_variance(losses, N=10):\n",
    "    \"\"\"\n",
    "    Plots the average and variance of training loss over every N steps.\n",
    "\n",
    "    Args:\n",
    "        losses (float): loss values to plot.\n",
    "        N (int): Number of steps to average over.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the means and variances over every N steps\n",
    "    means = []\n",
    "    variances = []\n",
    "\n",
    "    for i in range(0, len(losses), N):\n",
    "        # Handle the last data point if it's smaller than N\n",
    "        end = i + N\n",
    "        if end > len(losses) and len(losses[i:]) < N:\n",
    "            chunk = np.concatenate([losses[i - (N - len(losses[i:])) : i], losses[i:]])\n",
    "        else:\n",
    "            chunk = losses[i:end]\n",
    "\n",
    "        means.append(np.mean(chunk))\n",
    "        variances.append(np.var(chunk))\n",
    "\n",
    "    # Create an x-axis for the plot\n",
    "    x = np.arange(0, len(losses), N)[: len(means)]\n",
    "\n",
    "    # Plot the means and variances\n",
    "    plt.plot(x, means, \"-o\", label=\"Mean Loss\")\n",
    "    plt.fill_between(\n",
    "        x,\n",
    "        np.array(means) - np.array(variances),\n",
    "        np.array(means) + np.array(variances),\n",
    "        color=\"gray\",\n",
    "        alpha=0.2,\n",
    "        label=\"Variance\",\n",
    "    )\n",
    "\n",
    "    # Set the title and labels for the plot\n",
    "    plt.title(\"Training Loss over Time\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ee541a-ec8a-4cb2-a924-496f5276fa4c",
   "metadata": {},
   "source": [
    "In the following cell, the sub-kernel method is executed. The user can control a number of conditions and parameters such as:\n",
    "\n",
    "- The **optimizer choice** (SPSA, ADAM, GD, L-BFGS-B, NFT);\n",
    "- The **batch size**;\n",
    "- The **sub-kernel size** from *k* to full size;\n",
    "- *Optional* - balancing of the batches. Note that in this tutorial, the balancing funcion has not been declared.\n",
    "- The **initialisation points** in ‘init_p‘ (0 to 0.5). The closer to 0, the poorer is the starting point. At 0.5, the circuit acts as an Hadamard gate.\n",
    "\n",
    "### Task 5\n",
    "We'll start small, use a batch_size of just 1 and a sub_kernel_size of 4 and get the training loop to work once by following the sub-tasks.\n",
    "1. Initialise the `TrainableFidelityStatevectorKernel`.\n",
    "2. Initialise the `QuantumKernelTrainer` using the `BatchesSVCLoss` defined below.\n",
    "3. Fit the `QuantumKernelTrainer`. \n",
    "4. Retrieve the optimised kernel from the `QuantumKernelTrainerResult` returned from the `.fit()` method.\n",
    "5. Run the optimised kernel through the `run_QSVC` function. The training time should be a matter of seconds, if it takes any longer a mistake has been made.\n",
    "6. Once you've successfully completed steps 1-5, we can now run the full experiment, testing different batch sizes and sub kernel sizes. Run the settings below to retrieve the full experiment results.\n",
    "~~~\n",
    "optimizers = [\"SPSA\"]\n",
    "batch_sizes = [1, 4, 8]\n",
    "sub_kernel_sizes = [4, 8, 16, 32, None]\n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a190efef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Options\n",
    "\n",
    "optimizers = [\"SPSA\"]\n",
    "batch_sizes = [1]\n",
    "sub_kernel_sizes = [4]\n",
    "batch_types = [False]\n",
    "results = {}\n",
    "#init_p is the initialisation points. The closer to 0, the worst the start. 0.5 is optimial, the circuit acts as an hadamard gate.\n",
    "init_p = 0.25 * np.random.uniform(\n",
    "    -np.pi, np.pi, len(fm.training_parameters)\n",
    ")  \n",
    "### End Options\n",
    "\n",
    "for opt in optimizers:\n",
    "    opt_results = []\n",
    "    for sub_kernel_size in sub_kernel_sizes:\n",
    "        for batch_size in batch_sizes:\n",
    "            if sub_kernel_size == None:\n",
    "                if batch_size != 1:\n",
    "                    continue\n",
    "            for batch_type in batch_types:\n",
    "                if sub_kernel_size == None:\n",
    "                    sk_size = len(training_data)\n",
    "                    if batch_type == True:\n",
    "                        continue\n",
    "                else:\n",
    "                    sk_size = int(sub_kernel_size)\n",
    "                print(\"=\" * 50)\n",
    "                print(\"Training with \", opt, \" optimizer.\")\n",
    "                print(\n",
    "                    \"Currently using subkernel size: \",\n",
    "                    sub_kernel_size,\n",
    "                    \" where loss is averaged over \",\n",
    "                    batch_size,\n",
    "                    \" loss.\",\n",
    "                )\n",
    "                print(\"Sub-kernels prepared according to Balanced:\", batch_type)\n",
    "                print(\"=\" * 50)\n",
    "                # Get optimizer options\n",
    "                cb, optimizer = get_optimizer_options(opt)\n",
    "                if cb == \"SPSACallback\":\n",
    "                    callb = QKTCallback()\n",
    "                    optimizer = optimizer(callback=callb.callback)\n",
    "                else:\n",
    "                    optimizer = optimizer()\n",
    "                # Initialise quantum kernel\n",
    "                qk = \n",
    "\n",
    "                # Instantiate Sub-kernel loss\n",
    "                loss = BatchedSVCLoss(\n",
    "                    training_data,\n",
    "                    training_labels,\n",
    "                    minibatch_size=batch_size,\n",
    "                    sub_kernel_size=sub_kernel_size,\n",
    "                    balanced_batch=batch_type,\n",
    "                    shuffle=True,\n",
    "                    encoder=None,\n",
    "                )\n",
    "                # Instantiate a quantum kernel trainer.\n",
    "                qkt = \n",
    "\n",
    "                # Train the kernel\n",
    "                start = timer()\n",
    "                qka_results = \n",
    "                end = timer()\n",
    "                train_time = end - start\n",
    "                print(f\" Training Runtime: {train_time} secs. Results: \")\n",
    "                print()\n",
    "                # print('-'*80)\n",
    "                # print(qka_results)\n",
    "                # print('-'*80)\n",
    "                # print()\n",
    "                # print(\"Evaluating optimized kernel with the optimal parameters...\")\n",
    "                optimized_kernel = \n",
    "\n",
    "                start = timer()\n",
    "\n",
    "                # Train the QSVC using optimized quantum fidelity kernel\n",
    "                qsvc, auc, f1, accuracy = \n",
    "                end = timer()\n",
    "                qsvc_runtime = end - start\n",
    "                num_support_vectors = len(qsvc.support_)\n",
    "\n",
    "                print(f\"QSVC Training Runtime: {qsvc_runtime} secs\")\n",
    "\n",
    "                # Print results\n",
    "                print(\"-\" * 50)\n",
    "                print(\"F1 Score = %.3f\" % (f1))\n",
    "                print(\"ROC AUC = %.3f\" % (auc))\n",
    "                print(\"Accuracy Score = %.3f\" % (accuracy))\n",
    "                print()\n",
    "                print()\n",
    "\n",
    "                # Get the training loss\n",
    "                plot_data = (\n",
    "                    len(training_data)\n",
    "                    * np.array(loss.loss_arr)\n",
    "                    / (sk_size * num_support_vectors)\n",
    "                )\n",
    "\n",
    "                # Plotting\n",
    "                plot_average_loss_with_variance(plot_data, N=20)\n",
    "\n",
    "                # Append the results\n",
    "                opt_results.append(\n",
    "                    {\n",
    "                        \"ROC\": auc,\n",
    "                        \"F1\": f1,\n",
    "                        \"accuracy\": accuracy,\n",
    "                        \"sub_kernel_size\": sub_kernel_size,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"Balanced\": batch_type,\n",
    "                        \"train_time\": train_time,\n",
    "                        \"qsvc_runtime\": qsvc_runtime,\n",
    "                        \"training_loss\": plot_data.tolist(),\n",
    "                        \"loss\": loss.loss_arr,\n",
    "                        \"opt_params\": qka_results.optimal_point.tolist(),\n",
    "                    }\n",
    "                )\n",
    "    results.update({opt: opt_results})\n",
    "\n",
    "with open(\"Subkernel_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b22dcc-9857-4416-a8ac-7a6d5c15105f",
   "metadata": {},
   "source": [
    "### 4.c. Sub-Kernel Visualisation and Conclusion\n",
    "By looking over the graphs produced in the experiment, we can see a trend, the larger the sub-kernel and the greater the batch size the better the model performs. This is intuitively correct, we knew that running a QSVC using the full kernel worked very well. The interesting thing we can note is that around the 16 size kernel mark we are already able to start replicating the performance of the full kernel training whilst drastically decreasing training time. This is showing the method working, that we can approximate for the whole dataset using these sub-kernels. \n",
    "\n",
    "More formally, the results show us that using the sub-kernel method for variational training of kernels leads to the construction of an \"optimised\" full kernel that delivers a binary classification model that is better performing (better F1 Score, AUC ROC and accuracy) compared to the use of an \"un-optimised\" full kernel, with lower training times than optimising the full kernel. From a Sub Kernel size of 16 upwards we can see metric results that rival the full kernel, and increasingly lowering final training loss. See results in section 2 of the paper for further conclusions [[1]](https://arxiv.org/abs/2401.02879).\n",
    "\n",
    "Below there is a quick plotting function simply to show the training time increasing as the kernel size and batch size is increased. This is a simple way to see how the training time is decreased whilst keeping the metrics up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29798f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sub_kernel_experiments(results):\n",
    "    \"\"\"\n",
    "    Plotting function to show a bar graph of the training time increasing with sub-kernel size and number of kernels evaluated.\n",
    "    Keep in mind this fucntion is expecting the final sub kernel size to be None for comparison.\n",
    "    Args:\n",
    "       results (dict): plot data from the sub-kernel experiments.\n",
    "\n",
    "    Returns:\n",
    "       None\n",
    "\n",
    "    \"\"\"\n",
    "    for key in results.keys():\n",
    "        plot_data = results.get(key)\n",
    "        selected_data = np.ones((len(plot_data), 6))\n",
    "        for i in range(len(plot_data)):\n",
    "            line = np.ones(6)\n",
    "            line[0] = plot_data[i].get(\"ROC\")\n",
    "            line[1] = plot_data[i].get(\"F1\")\n",
    "            line[2] = plot_data[i].get(\"accuracy\")\n",
    "            line[3] = plot_data[i].get(\"sub_kernel_size\")\n",
    "            line[4] = plot_data[i].get(\"batch_size\")\n",
    "            line[5] = plot_data[i].get(\"train_time\")\n",
    "            selected_data[i] = line\n",
    "\n",
    "        bar_chart_data = np.ones((len(batch_sizes), len(sub_kernel_sizes) - 1, 6))\n",
    "        for j in range(len(batch_sizes)):\n",
    "            for k in range(len(sub_kernel_sizes) - 1):\n",
    "                bar_chart_data[j, k] = selected_data[len(batch_sizes) * k + j]\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(7, 5))\n",
    "        barWidth = 0.25\n",
    "        br = np.arange(len(sub_kernel_sizes) - 1)\n",
    "        for l in range(len(batch_sizes)):\n",
    "            label = \"batch size: \" + str(batch_sizes[l])\n",
    "            if l > 0:\n",
    "                br = [x + barWidth for x in br]\n",
    "            ax.bar(br, bar_chart_data[l][:, -1], width=barWidth, edgecolor=\"grey\", label=label)\n",
    "        ax.bar(\n",
    "            len(sub_kernel_sizes) - 1 + barWidth,\n",
    "            selected_data[-1, -1],\n",
    "            color=\"black\",\n",
    "            width=barWidth,\n",
    "            edgecolor=\"grey\",\n",
    "            label=\"Full kernel\",\n",
    "        )\n",
    "        ax.set_ylabel(\"Training time (s)\")\n",
    "        ax.set_xlabel(\"Sub kernel size\")\n",
    "        ax.set_xticks([r + barWidth for r in range(len(sub_kernel_sizes))], sub_kernel_sizes)\n",
    "        ax.set_title(\"Training time for sub kernel and batch sizes\")\n",
    "        ax.legend()\n",
    "\n",
    "    print([\"ROC\", \"F1\", \"Accuracy\", \"Sub kernel size\", \"Batch size\", \"Train time (s)\"])\n",
    "    print(selected_data)\n",
    "\n",
    "\n",
    "plot_sub_kernel_experiments(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29a89a",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Efficient Parameter Optimisation for Quantum Kernel Alignment: A Sub-sampling Approach in Variational Training, M. Emre Sahin and Benjamin C. B. Symons and Pushpak Pati and Fayyaz Minhas and Declan Millar and Maria Gabrani and Jan Lukas Robertus and Stefano Mensa, 2024, https://arxiv.org/abs/2401.02879\n",
    "2. Covariant quantum kernels for data with group structure, Glick, Jennifer R. and Gujarati, Tanvi P. and Córcoles, Antonio D. and Kim, Youngseok and Kandala, Abhinav and Gambetta, Jay M. and Temme, Kristan, 2024, https://arxiv.org/abs/2105.03406\n",
    "3. Quantum Kernel Training Toolkit, Jennifer R. Glick and Tanvi P. Gujarati, 2024, https://github.com/qiskit-community/prototype-quantum-kernel-training/tree/main/data\n",
    "4. Quantum Kernel Training Toolkit, Jennifer R. Glick and Tanvi P. Gujarati, 2024, https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/background/qkernels_and_data_w_group_structure.ipynb\n",
    "5. Quantum Kernel Training Toolkit, Jennifer R. Glick and Tanvi P. Gujarati, 2024, https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/how_tos/create_custom_quantum_feature_map.ipynb\n",
    "6. Quantum Kernel Training Toolkit, Jennifer R. Glick and Tanvi P. Gujarati, 2024, https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/how_tos/train_kernels_using_qiskit_feature_maps.ipynb\n",
    "7. The IBM Quantum heavy hex lattice, Paul Nation, Hanhee Paik, Andrew Cross, Zaira Nazario, 2021, https://www.ibm.com/quantum/blog/heavy-hex-lattice\n",
    "8. Quantum Kernel Training Toolkit, Jennifer R. Glick and Tanvi P. Gujarati, 2024, https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/background/svm_weighted_kernel_alignment.ipynb\n",
    "9. Qiskit Algorithms 0.3.0, 2024, https://qiskit-community.github.io/qiskit-algorithms/apidocs/qiskit_algorithms.optimizers.html\n",
    "10. Sequential minimal optimization for quantum-classical hybrid algorithms, Nakanishi, Ken M. and Fujii, Keisuke and Todo, Synge, 2020, https://arxiv.org/abs/1903.12166"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "rise": {
   "height": "90%",
   "scroll": true,
   "start_slideshow_at": "beginning",
   "theme": "white",
   "transition": "zoom",
   "width": "90%"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
